#import "../uestc-thesis-template/lib.typ":*

= 实验

== 基于多维度评估体系下的问答对生成质量对比实验

为了系统评估不同规模与架构的语言模型在学术文本问答对（QA对）生成任务中的性能差异，我设置了基于多维度评估体系下的QA对生成质量对比实验，用于探索模型参数规模、架构设计与生成质量之间的关联性，以便于优化知识密集型场景下的问答系统的生成质量。

=== 实验设置

*实验环境*
- 硬件环境
    - GPU：NVIDIA RTX 3090 24G
    - CPU：AMD EPYC 7642 48-Core Processor
    - 内存：64G
- 软件环境
    - 操作系统：Ubuntu 22.04.5 LTS
    - CUDA版本：12.4
    - 深度学习框架：PyTorch 2.5.1
    - python版本：3.12.3

*数据集&预处理*

本实验从UTD24系列期刊中选择了240篇文章作为实验源数据（独立于UTD24_QA数据集源），利用MinerU进行预处理将PDF转为Markdwon，我们将所有markdown文本内容截断至40000字符以内以便于适应后续模型处理要求。

*实验对象*

我们选择了三种QAG框架进行对比，括号中的为该框架使用到的大模型类别，由于SciQAG其本质为Prompt工程，依赖LLM本身在QAG任务中的表现，因此我们选择了8种不同参数规格的模型进行对比，以便于得出更客观的结果。

- MiniRAG-QAG框架（LLM：Qwen-plus）
- $"LLM"_("QAG")$（LLM：Qwen_v2.5_7b_Instruct）
- SciQAG框架（LLM：Deepseek-R1，Deepseek-v3，Qwen-Max，Qwen-Plus，Deepseek-R1-Distill系列（7B/14B），Llama3.3-70B-Instruct，QwQ-32B）


=== 实验流程

#picture-figure("QA质量评估框架，输入用于测评的QA数据和对应的源文档，输出五维度的得分。", 
        image("/src/pic/QAE.png",width: 100%)
        )<QAE>

在本实验中，我们构造了一体式的QA评估框架，如 @QAE 所示，输入文献和基于该文献生成的QA，框架会自动从相关性（Relevance）、不可知性（Agnosticism）、完整性（Completeness）、准确性（Accuracy）、合理性（Reasonableness）这五个方面进行综合评估并输出各项指标的分数，具体指标算子请查看 @QEM，评估流程如下：

1. QA生成：各框架利用预处理好的数据生成来生成QA，每篇文章生成10组QA，生成指令严格遵循预定义格式规范，温度系数均采用0.7来平衡探索与确定性。
2. 文献分块：该环节利用LangChain内的Lajavaness/bilingual-embedding-small模型对论文进行语义分块。
3. 向量化处理：利用BGE-M3模型对论文分块和QA对进行嵌入编码，生成1024维稠密向量。
4. 得分计算：通过预设指标计算流程，自动化完成各维度评分，输出结构化得分表。

=== QA评估结果与分析

#table-figure(
  "QA质量评估结果，总分为五个维度的平均分",
table(
    columns: 8,
    table.hline(stroke: 1pt),
    stroke: none,
    table.vline(
        x: 1,
        start: 0,
        end: 14,
        stroke:0.5pt
    ),
    table.header([QAG架构], [模型], [相关性], [不可知性], [完整性], [准确性], [合理性], [总分], ),

    table.hline(stroke: 0.5pt),

    table.cell(rowspan: 2,align: horizon)[MiniRAG], 
    [MiniRAG], [*0.6036*], [*0.8804*], [*0.6871*], [*0.6935*], [*0.8487*], [*0.7427*], 
    [$"LLM"_("QAG")$-7b], [0.6032], [0.6802], [0.6440], [0.5191], [0.8237], [0.6541], 

    table.hline(stroke: 0.5pt),

    table.cell(rowspan: 8,align: horizon)[SciQAG], 
    [qwen_max ], [0.6013], [0.7417], [0.6699], [0.6341], [0.7991], [0.7058], 
    [qwen_plus], [0.5955], [0.7724], [0.6484], [0.5485], [0.8309], [0.6721], 
    [deepseek-R1], [0.5894], [0.8712], [0.6200], [0.4522], [0.8319], [0.6730], 
    [deepseek-v3], [0.5972], [0.7942], [0.6623], [0.6279], [0.8334], [0.7030], 
    [ds-r1-qwen-7b ], [0.5771], [0.5718], [0.6038], [0.3705], [0.8080], [0.5863], 
    [ds-r1-qwen-14b], [0.5876], [0.8008], [0.6396], [0.5370], [0.8318], [0.6794], 
    [llama3.3-70b], [0.5945], [0.7531], [0.6566], [0.5791], [0.8198], [0.6806], 
    [QwQ-32b], [0.5915], [0.8316], [0.6296], [0.5113], [0.8383], [0.6805], 

    table.hline(stroke: 1pt),
    )
)

// 本实验通过系统评估揭示了不同模型在学术论文QA生成任务中的差异化表现，根据评估情况，MiniRAG架构在整体质量上保持显著优势，其基础模型以0.7773总分领跑所有测试对象，尤其在准确性（0.7203）与不可知性（0.8804）维度建立技术壁垒，较次优模型分别高出0.8%和1.06%。值得注意的是，经领域微调的$"LLM"_"QAG"$（7B）在同参数量级对比中展现优越性，其总分（0.6870）较同等规模的ds-r1-qwen-7b（0.6122）提升12.25%，且在相关性（0.5903 vs 0.3418）和合理性（0.9207 vs 0.8233）维度实现72.8%和11.8%的显著进步，证明针对性微调能有效提升模型的任务适配能力。

// SciQAG架构下的qwen_max（总分0.7097）与deepseek-v3（0.6930）虽在总分逼近$"LLM"_"QAG"$（0.6870），但其核心质量指标呈现显著结构性差异。qwen_max的准确性（0.6381）较MiniRAG基准值（0.7203）低11.4%，而deepseek-v3的不可知性（0.7942）落后基准模型9.8个百分点。相较之下，$"LLM"_"QAG"$在关键质量维度展现出更好的均衡性——其合理性（0.9207）与完整性（0.6440）分别达到基准模型的92.4%和93.7%，且相关性（0.5903）较qwen_max（0.5355）提升10.2%。

本实验通过系统评估揭示了不同模型在学术论文QA生成任务中的差异化表现。根据评估情况，MiniRAG架构下的MiniRAG模型在整体质量上保持显著优势，其以*0.7427*的总分领跑所有测试模型，尤其在准确性（0.6935）维度表现突出，较次优提升近9.37%。经领域微调的$"LLM"_("QAG")-7$b在同参数量级对比中展现优越性，其总分（0.6541）较同等规模的ds-r1-qwen-7b（0.5863）提升约11.5%，且在相关性（0.6032 vs 0.5771）提升约4.5%，合理性（0.8237 vs 0.8080）提升约1.9%，证明针对性微调能有效提升模型的任务适配能力。  

SciQAG架构下的qwen_max（总分0.7058）与deepseek-v3（0.7030）虽在总分接近，但核心质量指标呈现显著结构性差异。qwen_max的准确性（0.6341）较MiniRAG（0.6935）低约8.6%，而deepseek-v3的不可知性（0.7942）落后MiniRAG（0.8804）约9.8个百分点。相较之下，$"LLM"_("QAG")-7$b在关键质量维度展现出一定的均衡性和能效优势，仅7b参数可以媲美qwen_max和deepseek-R1这一类超大型模型。

== 评估方法有效性实验

在 @QEM 本文提出了包含相关性、不可知性、完整性、准确性、合理性的QA综合评估体系，为了系统评估所采用方法在输入扰动和超参数变化下的稳健性，本节将开展一系列灵敏度实验。

本节从两个方面探讨评估方法的敏感程度：

1. 通过调整文本分块数量的变化，以模拟不同输入粒度对模型评估排名的影响。
2. 通过调整不可知性权重参数，以衡量各模型对该超参数变化的响应程度及排名稳定性。

通过对比不同设置下的Kendall Tau @sen1968estimates 与Spearman Rho 秩相关系数来量化评估结果的变化趋势，从而检验评估方法的稳健性。

=== 文本分块灵敏度测试

为评估评估方法对输入粒度变化的稳定性，本文测试了在不同文本分块数量（5、10、15、20）下，各模型的评估排名，并计算排名之间的Kendall Tau和Spearman Rho秩相关系数。结果如 @chunks 所示。

#picture-figure("文本分块灵敏测试结果。左：不同分块数量排名间的Spearman Rho秩相关系数，右：不同分块数量排名间的Kendall Tau。", 
        image("/src/pic/chunks.png",width: 100%)
        )<chunks>

如图，Kendall Tau系数整体较高（多数在0.78以上），说明评估排名在文本分块数发生变化时具有较强的秩序一致性。其中，5与15之间的Kendall Tau系数达到0.909，表明这两种分块方式得到的排名高度一致；而5与20之间的系数为0.697，显示出一定程度的不稳定；Spearman Rho系数普遍高于Kendall Tau（多为0.88以上），说明虽然个别模型排名发生了小幅变化，但整体排名顺序的线性相关性仍较强，评估结果在不同粒度下具备较好的稳健性。

总体来看，文本分块数量的调整会对评估排名带来一定影响，尤其在极端粒度差异下（如5 vs. 20分块），但整体表现出中等偏高的稳定性，验证了该评估方法对输入扰动具有一定的鲁棒性。

=== 不可知性权重灵敏度测试

本文进一步分析了评估方法对不可知性加权参数的敏感性。通过在BLEU得分与违禁词惩罚得分之间以不同权重组合（从[0.1, 0.9]至[0.9, 0.1]）计算综合评分，记录了各模型排名随权重变化的情况，并据此构造秩相关矩阵，随着权重变化而改变，模型的不可知性得分如 @unawerness  所示。


#picture-figure("BLEU和禁词惩罚得分权重变化对QA不可知性评估的影响。横坐标为BLEU的权重变化。", 
        image("/src/pic/unawerness.png",width: 100%)
        )<unawerness>

一些模型（如 deepseek-R1, qwq_32b）在参数变化下排名保持稳定，表明其性能在不同评价重点下均较为均衡；而其他模型如 qwen_max、llama3.3-70b-instruct 等在权重从偏向BLEU切换至偏向惩罚时排名大幅下降，表现出较强的参数敏感性。为了找到BLEU得分和违禁词惩罚得分之间最佳的权重，本文计算了不同权重下模型排名之间的Kendall Tau和Spearman Rho秩相关系数分数，如 @unawer_hot 所示。

#picture-figure("不同权重下模型排名之间的Kendall Tau和Spearman Rho秩相关系数分数热力图。", 
        image("/src/pic/unawer_hot.png",width: 100%)
        )<unawer_hot>
Kendall Tau 结果分析：不可知性在中间权重区间（0.4–0.6）是非常稳定的，极端权重设置容易导致不可知性排名大幅波动。
Spearman Rho 结果分析：整体趋势基本保持，极端权重变化下也不是彻底乱序，只是细节局部变化多。

从Kendall Tau和Spearman Rho的热力图来看，参数相近的权重组合（如[0.4, 0.6]与[0.5, 0.5]）之间秩相关性很高（Kendall Tau > 0.9），而参数差异较大的组合（如[0.1, 0.9]与[0.9, 0.1]）之间的秩相关性明显下降（Kendall Tau约为0.27，Spearman Rho约为0.39），说明不可知性评估排名对该权重具有一定程度的敏感性，为了保证评估系统的均衡，本文的不可知性评估中，BLEU得分和违禁词惩罚得分的权重设为[0.4, 0.6]。